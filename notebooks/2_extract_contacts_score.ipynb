{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6140f0d-4e8a-4aec-9cff-666352030eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('instagram_leads.db')  \n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS profiles (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    username TEXT UNIQUE,\n",
    "    email TEXT,\n",
    "    phone TEXT,\n",
    "    website_link TEXT,\n",
    "    follower_count INTEGER,\n",
    "    location TEXT,\n",
    "    lead_score INTEGER,\n",
    "    date_scraped TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\" Table `profiles` ensured at ./instagram_leads.db\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23092c-900c-4032-beaf-ec0ca5609e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "import time, sqlite3, zipfile, random, re, json\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "\n",
    "# --- SCORING FUNCTION ---\n",
    "def score_lead(bio, email, phone, follower_count):\n",
    "    score = 0\n",
    "    if 1000000 <= follower_count <= 10000000:\n",
    "        score += 60\n",
    "    elif 10000 <= follower_count <= 1000000:\n",
    "        score += 50\n",
    "    elif 1000 <= follower_count <= 10000:\n",
    "        score += 40\n",
    "    elif 500 <= follower_count < 1000:\n",
    "        score += 30\n",
    "    elif 100 <= follower_count < 500:\n",
    "        score += 20\n",
    "    elif follower_count > 0:\n",
    "        score += 10\n",
    "    \n",
    "    if email:\n",
    "        score += 20\n",
    "    if phone:\n",
    "        score += 20\n",
    "    \n",
    "    keywords = ['lash tech', 'lashes', 'certified', 'pmu', 'brow', 'beauty', 'studio']\n",
    "    if any(word in bio.lower() for word in keywords):\n",
    "        score += 10\n",
    "    \n",
    "    return score\n",
    "\n",
    "# --- ENHANCED CONTACT EXTRACTION ---\n",
    "def extract_contact_info(bio, external_url=None):\n",
    "    email = re.search(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', bio or \"\")\n",
    "    phone = re.search(r'\\+?\\d[\\d\\s\\-()]{7,}', bio or \"\")\n",
    "\n",
    "    website = None\n",
    "    if external_url:\n",
    "        website = external_url\n",
    "    else:\n",
    "        match_https = re.search(r'(https?://[^\\s]+)', bio or \"\")\n",
    "        if match_https:\n",
    "            website = match_https.group(0)\n",
    "        else:\n",
    "            linkinbio_services = [\n",
    "                \"linktr.ee\", \"beacons.ai\", \"stan.store\", \"solo.to\", \"carrd.co\",\n",
    "                \"withkoji.com\", \"taplink.cc\", \"flow.page\", \"msha.ke\", \"bio.site\", \"linkin.bio\"\n",
    "            ]\n",
    "            for service in linkinbio_services:\n",
    "                match_service = re.search(rf'({service}/[^\\s]+)', bio or \"\", re.IGNORECASE)\n",
    "                if match_service:\n",
    "                    website = f\"https://{match_service.group(1)}\"\n",
    "                    break\n",
    "            if not website:\n",
    "                match_domain = re.search(\n",
    "                    r'\\b[\\w.-]+\\.(com|ca|net|org|studio|beauty|store|io|facebook)\\b',\n",
    "                    bio or \"\", re.IGNORECASE\n",
    "                )\n",
    "                if match_domain:\n",
    "                    website = f\"https://{match_domain.group(0)}\"\n",
    "\n",
    "    return (\n",
    "        email.group(0) if email else None,\n",
    "        phone.group(0) if phone else None,\n",
    "        website\n",
    "    )\n",
    "\n",
    "# --- ENHANCED LOCATION DETECTION ---\n",
    "def normalize_text_for_location(text):\n",
    "    \"\"\"Normalize Unicode text to ASCII while preserving meaning\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # First normalize Unicode characters (e.g., 𝑹𝒊𝒄𝒉𝒎𝒐𝒏𝒅 -> Richmond)\n",
    "    normalized = unicodedata.normalize('NFKD', text)\n",
    "    ascii_text = normalized.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Keep letters, numbers, spaces, commas, and basic punctuation\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9\\s,\\.-]', ' ', ascii_text)\n",
    "    \n",
    "    # Clean up extra spaces\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Comprehensive Canadian location database\n",
    "canadian_locations = {\n",
    "    # British Columbia - Major Cities\n",
    "    \"vancouver\": \"Vancouver, BC\", \"van\": \"Vancouver, BC\", \"vancity\": \"Vancouver, BC\",\n",
    "    \"surrey\": \"Surrey, BC\", \"burnaby\": \"Burnaby, BC\", \"richmond\": \"Richmond, BC\",\n",
    "    \"coquitlam\": \"Coquitlam, BC\", \"port coquitlam\": \"Port Coquitlam, BC\", \"poco\": \"Port Coquitlam, BC\",\n",
    "    \"port moody\": \"Port Moody, BC\", \"langley\": \"Langley, BC\", \"delta\": \"Delta, BC\",\n",
    "    \"north vancouver\": \"North Vancouver, BC\", \"north van\": \"North Vancouver, BC\", \"northvan\": \"North Vancouver, BC\",\n",
    "    \"west vancouver\": \"West Vancouver, BC\", \"west van\": \"West Vancouver, BC\", \"westvancouver\": \"West Vancouver, BC\",\n",
    "    \"new westminster\": \"New Westminster, BC\", \"newwest\": \"New Westminster, BC\", \"newwestminster\": \"New Westminster, BC\",\n",
    "    \"abbotsford\": \"Abbotsford, BC\", \"chilliwack\": \"Chilliwack, BC\", \"maple ridge\": \"Maple Ridge, BC\",\n",
    "    \"white rock\": \"White Rock, BC\", \"whiterock\": \"White Rock, BC\",\n",
    "    \n",
    "    # BC - Other Cities\n",
    "    \"victoria\": \"Victoria, BC\", \"vic\": \"Victoria, BC\", \"kelowna\": \"Kelowna, BC\", \"kamloops\": \"Kamloops, BC\",\n",
    "    \"nanaimo\": \"Nanaimo, BC\", \"prince george\": \"Prince George, BC\", \"vernon\": \"Vernon, BC\",\n",
    "    \"penticton\": \"Penticton, BC\", \"squamish\": \"Squamish, BC\", \"whistler\": \"Whistler, BC\",\n",
    "    \"fort st john\": \"Fort St. John, BC\", \"fort st. john\": \"Fort St. John, BC\", \"fsj\": \"Fort St. John, BC\",\n",
    "    \"williams lake\": \"Williams Lake, BC\", \"quesnel\": \"Quesnel, BC\", \"terrace\": \"Terrace, BC\",\n",
    "    \"dawson creek\": \"Dawson Creek, BC\", \"salmon arm\": \"Salmon Arm, BC\", \"campbell river\": \"Campbell River, BC\",\n",
    "    \"courtenay\": \"Courtenay, BC\", \"powell river\": \"Powell River, BC\", \"ladysmith\": \"Ladysmith, BC\",\n",
    "    \"parksville\": \"Parksville, BC\",\n",
    "    \n",
    "    # BC - Regions and Abbreviations\n",
    "    \"bc\": \"British Columbia\", \"british columbia\": \"British Columbia\", \"yvr\": \"Vancouver, BC\",\n",
    "    \"lower mainland\": \"Lower Mainland, BC\", \"fraser valley\": \"Fraser Valley, BC\",\n",
    "    \"okanagan\": \"Okanagan, BC\", \"van island\": \"Vancouver Island, BC\", \"vancouver island\": \"Vancouver Island, BC\",\n",
    "    \"sunshine coast\": \"Sunshine Coast, BC\", \"sea to sky\": \"Sea to Sky, BC\",\n",
    "\n",
    "    # Alberta - Major Cities\n",
    "    \"calgary\": \"Calgary, AB\", \"yyc\": \"Calgary, AB\", \"cowtown\": \"Calgary, AB\",\n",
    "    \"edmonton\": \"Edmonton, AB\", \"yeg\": \"Edmonton, AB\", \"e-town\": \"Edmonton, AB\", \"etown\": \"Edmonton, AB\",\n",
    "    \"red deer\": \"Red Deer, AB\", \"reddeer\": \"Red Deer, AB\",\n",
    "    \"sherwood park\": \"Sherwood Park, AB\", \"sherwoodpark\": \"Sherwood Park, AB\",\n",
    "    \"st albert\": \"St. Albert, AB\", \"st. albert\": \"St. Albert, AB\", \"stalbert\": \"St. Albert, AB\",\n",
    "    \n",
    "    # Alberta - Other Cities\n",
    "    \"fort mcmurray\": \"Fort McMurray, AB\", \"fort mac\": \"Fort McMurray, AB\", \"ftmac\": \"Fort McMurray, AB\",\n",
    "    \"lethbridge\": \"Lethbridge, AB\", \"medicine hat\": \"Medicine Hat, AB\", \"medhat\": \"Medicine Hat, AB\",\n",
    "    \"grande prairie\": \"Grande Prairie, AB\", \"gp\": \"Grande Prairie, AB\", \"grandeprairie\": \"Grande Prairie, AB\",\n",
    "    \"airdrie\": \"Airdrie, AB\", \"spruce grove\": \"Spruce Grove, AB\", \"spruceglove\": \"Spruce Grove, AB\",\n",
    "    \"leduc\": \"Leduc, AB\", \"fort saskatchewan\": \"Fort Saskatchewan, AB\", \"fortsask\": \"Fort Saskatchewan, AB\",\n",
    "    \"cochrane\": \"Cochrane, AB\", \"camrose\": \"Camrose, AB\", \"wetaskiwin\": \"Wetaskiwin, AB\",\n",
    "    \"cold lake\": \"Cold Lake, AB\", \"lloydminster\": \"Lloydminster, AB\", \"lloydmin\": \"Lloydminster, AB\",\n",
    "    \"okotoks\": \"Okotoks, AB\", \"canmore\": \"Canmore, AB\", \"jasper\": \"Jasper, AB\", \"banff\": \"Banff, AB\",\n",
    "    \n",
    "    # Alberta - Abbreviations and Regions\n",
    "    \"ab\": \"Alberta\", \"alberta\": \"Alberta\", \"yyc area\": \"Calgary, AB\", \"yeg area\": \"Edmonton, AB\",\n",
    "\n",
    "    # Ontario - Greater Toronto Area (GTA)\n",
    "    \"toronto\": \"Toronto, ON\", \"tdot\": \"Toronto, ON\", \"the six\": \"Toronto, ON\", \"yyz\": \"Toronto, ON\",\n",
    "    \"scarborough\": \"Scarborough, ON\", \"etobicoke\": \"Etobicoke, ON\", \"north york\": \"North York, ON\",\n",
    "    \"mississauga\": \"Mississauga, ON\", \"sauga\": \"Mississauga, ON\", \"brampton\": \"Brampton, ON\",\n",
    "    \"vaughan\": \"Vaughan, ON\", \"markham\": \"Markham, ON\", \"richmond hill\": \"Richmond Hill, ON\",\n",
    "    \"oakville\": \"Oakville, ON\", \"burlington\": \"Burlington, ON\", \"hamilton\": \"Hamilton, ON\",\n",
    "    \"ajax\": \"Ajax, ON\", \"whitby\": \"Whitby, ON\", \"pickering\": \"Pickering, ON\", \"oshawa\": \"Oshawa, ON\",\n",
    "    \"milton\": \"Milton, ON\", \"georgetown\": \"Georgetown, ON\", \"newmarket\": \"Newmarket, ON\",\n",
    "    \n",
    "    # Ontario - Other Major Cities\n",
    "    \"ottawa\": \"Ottawa, ON\", \"yow\": \"Ottawa, ON\", \"bytown\": \"Ottawa, ON\",\n",
    "    \"kanata\": \"Kanata, ON\", \"nepean\": \"Nepean, ON\", \"orleans\": \"Orleans, ON\", \"barrhaven\": \"Barrhaven, ON\",\n",
    "    \"london\": \"London, ON\", \"windsor\": \"Windsor, ON\", \"kingston\": \"Kingston, ON\",\n",
    "    \"kitchener\": \"Kitchener, ON\", \"waterloo\": \"Waterloo, ON\", \"cambridge\": \"Cambridge, ON\",\n",
    "    \"guelph\": \"Guelph, ON\", \"st catharines\": \"St. Catharines, ON\", \"st. catharines\": \"St. Catharines, ON\",\n",
    "    \"stcatharines\": \"St. Catharines, ON\", \"barrie\": \"Barrie, ON\", \"sudbury\": \"Sudbury, ON\",\n",
    "    \"thunder bay\": \"Thunder Bay, ON\", \"thunderbay\": \"Thunder Bay, ON\", \"brantford\": \"Brantford, ON\",\n",
    "    \"peterborough\": \"Peterborough, ON\", \"belleville\": \"Belleville, ON\", \"cornwall\": \"Cornwall, ON\",\n",
    "    \"sault ste marie\": \"Sault Ste. Marie, ON\", \"sault ste. marie\": \"Sault Ste. Marie, ON\",\n",
    "    \"sault\": \"Sault Ste. Marie, ON\", \"soo\": \"Sault Ste. Marie, ON\", \"welland\": \"Welland, ON\",\n",
    "    \"sarnia\": \"Sarnia, ON\", \"stratford\": \"Stratford, ON\", \"orillia\": \"Orillia, ON\",\n",
    "    \"timmins\": \"Timmins, ON\", \"north bay\": \"North Bay, ON\", \"northbay\": \"North Bay, ON\",\n",
    "    \"niagara falls\": \"Niagara Falls, ON\", \"niagarafalls\": \"Niagara Falls, ON\",\n",
    "    \n",
    "    # Ontario - Regions and Abbreviations\n",
    "    \"on\": \"Ontario\", \"ontario\": \"Ontario\", \"gta\": \"Greater Toronto Area, ON\",\n",
    "    \"golden horseshoe\": \"Golden Horseshoe, ON\", \"niagara region\": \"Niagara Region, ON\",\n",
    "    \"muskoka\": \"Muskoka, ON\", \"cottage country\": \"Cottage Country, ON\",\n",
    "\n",
    "    # Quebec - Major Cities\n",
    "    \"montreal\": \"Montreal, QC\", \"mtl\": \"Montreal, QC\", \"ville marie\": \"Montreal, QC\",\n",
    "    \"quebec city\": \"Quebec City, QC\", \"quebec\": \"Quebec City, QC\", \"qc city\": \"Quebec City, QC\",\n",
    "    \"ville de quebec\": \"Quebec City, QC\", \"laval\": \"Laval, QC\", \"longueuil\": \"Longueuil, QC\",\n",
    "    \"gatineau\": \"Gatineau, QC\", \"sherbrooke\": \"Sherbrooke, QC\", \"trois rivieres\": \"Trois-Rivières, QC\",\n",
    "    \"trois-rivieres\": \"Trois-Rivières, QC\", \"trois rivières\": \"Trois-Rivières, QC\",\n",
    "    \n",
    "    # Quebec - Other Cities\n",
    "    \"saint jean sur richelieu\": \"Saint-Jean-sur-Richelieu, QC\", \"saint-jean-sur-richelieu\": \"Saint-Jean-sur-Richelieu, QC\",\n",
    "    \"beloeil\": \"Beloeil, QC\", \"saguenay\": \"Saguenay, QC\", \"levis\": \"Lévis, QC\", \"lévis\": \"Lévis, QC\",\n",
    "    \"saint jerome\": \"Saint-Jérôme, QC\", \"saint-jerome\": \"Saint-Jérôme, QC\", \"st jerome\": \"Saint-Jérôme, QC\",\n",
    "    \"drummondville\": \"Drummondville, QC\", \"granby\": \"Granby, QC\", \"shawinigan\": \"Shawinigan, QC\",\n",
    "    \"chicoutimi\": \"Chicoutimi, QC\", \"rimouski\": \"Rimouski, QC\", \"rouyn noranda\": \"Rouyn-Noranda, QC\",\n",
    "    \"sept iles\": \"Sept-Îles, QC\", \"val d or\": \"Val-d'Or, QC\", \"alma\": \"Alma, QC\",\n",
    "    \n",
    "    # Quebec - Abbreviations and Regions\n",
    "    \"qc\": \"Quebec\", \"québec\": \"Quebec\", \"pq\": \"Quebec\", \"province de quebec\": \"Quebec\",\n",
    "\n",
    "    # Manitoba - Cities\n",
    "    \"winnipeg\": \"Winnipeg, MB\", \"peg\": \"Winnipeg, MB\", \"the peg\": \"Winnipeg, MB\",\n",
    "    \"brandon\": \"Brandon, MB\", \"steinbach\": \"Steinbach, MB\", \"thompson\": \"Thompson, MB\",\n",
    "    \"portage la prairie\": \"Portage la Prairie, MB\", \"portage\": \"Portage la Prairie, MB\",\n",
    "    \"winkler\": \"Winkler, MB\", \"selkirk\": \"Selkirk, MB\", \"dauphin\": \"Dauphin, MB\",\n",
    "    \"morden\": \"Morden, MB\", \"flin flon\": \"Flin Flon, MB\", \"the pas\": \"The Pas, MB\",\n",
    "    \n",
    "    # Manitoba - Abbreviations\n",
    "    \"mb\": \"Manitoba\", \"manitoba\": \"Manitoba\", \"ywg\": \"Winnipeg, MB\",\n",
    "\n",
    "    # Saskatchewan - Cities\n",
    "    \"regina\": \"Regina, SK\", \"yqr\": \"Regina, SK\", \"saskatoon\": \"Saskatoon, SK\", \"yxe\": \"Saskatoon, SK\",\n",
    "    \"moose jaw\": \"Moose Jaw, SK\", \"mjaw\": \"Moose Jaw, SK\", \"prince albert\": \"Prince Albert, SK\",\n",
    "    \"swift current\": \"Swift Current, SK\", \"swiftcurrent\": \"Swift Current, SK\", \"yorkton\": \"Yorkton, SK\",\n",
    "    \"north battleford\": \"North Battleford, SK\", \"estevan\": \"Estevan, SK\", \"weyburn\": \"Weyburn, SK\",\n",
    "    \"warman\": \"Warman, SK\", \"martensville\": \"Martensville, SK\", \"melville\": \"Melville, SK\",\n",
    "    \n",
    "    # Saskatchewan - Abbreviations\n",
    "    \"sk\": \"Saskatchewan\", \"saskatchewan\": \"Saskatchewan\",\n",
    "\n",
    "    # Nova Scotia - Cities\n",
    "    \"halifax\": \"Halifax, NS\", \"hal\": \"Halifax, NS\", \"hfx\": \"Halifax, NS\", \"dartmouth\": \"Dartmouth, NS\",\n",
    "    \"sydney\": \"Sydney, NS\", \"glace bay\": \"Glace Bay, NS\", \"new glasgow\": \"New Glasgow, NS\",\n",
    "    \"truro\": \"Truro, NS\", \"amherst\": \"Amherst, NS\", \"yarmouth\": \"Yarmouth, NS\",\n",
    "    \"kentville\": \"Kentville, NS\", \"antigonish\": \"Antigonish, NS\", \"bridgewater\": \"Bridgewater, NS\",\n",
    "    \"wolfville\": \"Wolfville, NS\", \"lower sackville\": \"Lower Sackville, NS\", \"sackville\": \"Sackville, NS\",\n",
    "    \"bedford\": \"Bedford, NS\", \"cole harbour\": \"Cole Harbour, NS\", \"eastern passage\": \"Eastern Passage, NS\",\n",
    "    \n",
    "    # Nova Scotia - Abbreviations and Regions\n",
    "    \"ns\": \"Nova Scotia\", \"nova scotia\": \"Nova Scotia\", \"hrm\": \"Halifax Regional Municipality, NS\",\n",
    "    \"cape breton\": \"Cape Breton, NS\", \"south shore\": \"South Shore, NS\", \"annapolis valley\": \"Annapolis Valley, NS\",\n",
    "\n",
    "    # New Brunswick - Cities\n",
    "    \"moncton\": \"Moncton, NB\", \"saint john\": \"Saint John, NB\", \"st john\": \"Saint John, NB\",\n",
    "    \"fredericton\": \"Fredericton, NB\", \"bathurst\": \"Bathurst, NB\", \"miramichi\": \"Miramichi, NB\",\n",
    "    \"edmundston\": \"Edmundston, NB\", \"campbellton\": \"Campbellton, NB\", \"sussex\": \"Sussex, NB\",\n",
    "    \"woodstock\": \"Woodstock, NB\", \"grand falls\": \"Grand Falls, NB\", \"oromocto\": \"Oromocto, NB\",\n",
    "    \n",
    "    # New Brunswick - Abbreviations\n",
    "    \"nb\": \"New Brunswick\", \"new brunswick\": \"New Brunswick\",\n",
    "\n",
    "    # Prince Edward Island - Cities\n",
    "    \"charlottetown\": \"Charlottetown, PE\", \"summerside\": \"Summerside, PE\", \"stratford\": \"Stratford, PE\",\n",
    "    \"cornwall\": \"Cornwall, PE\", \"montague\": \"Montague, PE\", \"kensington\": \"Kensington, PE\",\n",
    "    \"alberton\": \"Alberton, PE\", \"souris\": \"Souris, PE\", \"tignish\": \"Tignish, PE\",\n",
    "    \n",
    "    # Prince Edward Island - Abbreviations\n",
    "    \"pe\": \"Prince Edward Island\", \"pei\": \"Prince Edward Island\", \"prince edward island\": \"Prince Edward Island\",\n",
    "\n",
    "    # Newfoundland and Labrador - Cities\n",
    "    \"st johns\": \"St. John's, NL\", \"st. johns\": \"St. John's, NL\", \"saint johns\": \"St. John's, NL\",\n",
    "    \"corner brook\": \"Corner Brook, NL\", \"mount pearl\": \"Mount Pearl, NL\", \"conception bay south\": \"Conception Bay South, NL\",\n",
    "    \"paradise\": \"Paradise, NL\", \"grand falls windsor\": \"Grand Falls-Windsor, NL\", \"happy valley goose bay\": \"Happy Valley-Goose Bay, NL\",\n",
    "    \"gander\": \"Gander, NL\", \"stephenville\": \"Stephenville, NL\", \"bay roberts\": \"Bay Roberts, NL\",\n",
    "    \n",
    "    # Newfoundland and Labrador - Abbreviations\n",
    "    \"nl\": \"Newfoundland and Labrador\", \"nfld\": \"Newfoundland and Labrador\", \"newfoundland\": \"Newfoundland and Labrador\",\n",
    "    \"labrador\": \"Newfoundland and Labrador\", \"the rock\": \"Newfoundland and Labrador\",\n",
    "\n",
    "    # Territories\n",
    "    \"whitehorse\": \"Whitehorse, YT\", \"yellowknife\": \"Yellowknife, NT\", \"iqaluit\": \"Iqaluit, NU\",\n",
    "    \"yukon\": \"Yukon\", \"yt\": \"Yukon\", \"northwest territories\": \"Northwest Territories\", \"nwt\": \"Northwest Territories\",\n",
    "    \"nunavut\": \"Nunavut\", \"nu\": \"Nunavut\", \"dawson city\": \"Dawson City, YT\", \"watson lake\": \"Watson Lake, YT\",\n",
    "    \"hay river\": \"Hay River, NT\", \"inuvik\": \"Inuvik, NT\", \"fort smith\": \"Fort Smith, NT\",\n",
    "    \"rankin inlet\": \"Rankin Inlet, NU\", \"arviat\": \"Arviat, NU\", \"baker lake\": \"Baker Lake, NU\",\n",
    "    \n",
    "    # Regional Terms\n",
    "    \"maritimes\": \"Maritimes\", \"atlantic canada\": \"Atlantic Canada\", \"western canada\": \"Western Canada\",\n",
    "    \"central canada\": \"Central Canada\", \"northern canada\": \"Northern Canada\", \"prairies\": \"Prairie Provinces\",\n",
    "    \"eastern canada\": \"Eastern Canada\", \"french canada\": \"Quebec\", \"english canada\": \"English Canada\",\n",
    "    \n",
    "    # Common Variations and Nicknames\n",
    "    \"tdot\": \"Toronto, ON\", \"the hammer\": \"Hamilton, ON\", \"k town\": \"Kingston, ON\", \"the peg\": \"Winnipeg, MB\",\n",
    "    \"cow town\": \"Calgary, AB\", \"city of champions\": \"Edmonton, AB\", \"steel city\": \"Hamilton, ON\",\n",
    "    \"royal city\": \"Guelph, ON\", \"forest city\": \"London, ON\", \"oil city\": \"Sarnia, ON\",\n",
    "    \"polar bear capital\": \"Churchill, MB\", \"garden city\": \"St. Catharines, ON\", \"rose city\": \"Welland, ON\",\n",
    "    \"friendly city\": \"Moose Jaw, SK\", \"queen city\": \"Regina, SK\", \"bridge city\": \"Saskatoon, SK\",\n",
    "    \"festival city\": \"Edmonton, AB\", \"stampede city\": \"Calgary, AB\", \"ocean playground\": \"Nova Scotia\",\n",
    "    \"picture province\": \"New Brunswick\", \"spud island\": \"Prince Edward Island\", \"the rock\": \"Newfoundland\"\n",
    "}\n",
    "\n",
    "def enhanced_location_detection(bio, display_name, username, all_text=None):\n",
    "    \"\"\"Robust location detection with Unicode support\"\"\"\n",
    "    \n",
    "    # Combine all text sources\n",
    "    text_sources = [bio or \"\", display_name or \"\", username or \"\", all_text or \"\"]\n",
    "    combined_text = \" \".join(text_sources).strip()\n",
    "    \n",
    "    # Add debug logging\n",
    "    print(f\"🔍 Location Detection Debug for {username or 'unknown'}:\")\n",
    "    print(f\"   Bio: {repr(bio)}\")\n",
    "    print(f\"   Combined text: {repr(combined_text[:200])}\")\n",
    "    \n",
    "    if not combined_text:\n",
    "        print(\"   No text to analyze\")\n",
    "        return None\n",
    "    \n",
    "    # Normalize the text to handle Unicode characters\n",
    "    normalized_text = normalize_text_for_location(combined_text)\n",
    "    print(f\"   Normalized: {repr(normalized_text[:200])}\")\n",
    "    \n",
    "    # Convert to lowercase for matching\n",
    "    search_text = normalized_text.lower()\n",
    "    \n",
    "    # PRIORITY 1: Look for exact \"City, Province\" or \"City Province\" patterns\n",
    "    city_province_patterns = [\n",
    "        r'\\b([a-zA-Z\\s]+),\\s*(BC|AB|SK|MB|ON|QC|NB|NS|PE|NL|YT|NT|NU)\\b',\n",
    "        r'\\b([a-zA-Z\\s]+)\\s+(BC|AB|SK|MB|ON|QC|NB|NS|PE|NL|YT|NT|NU)\\b'\n",
    "    ]\n",
    "    \n",
    "    for pattern in city_province_patterns:\n",
    "        matches = re.findall(pattern, normalized_text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            if len(match) == 2:\n",
    "                city = match[0].strip().lower()\n",
    "                province = match[1].upper()\n",
    "                \n",
    "                # Try different combinations\n",
    "                combinations = [\n",
    "                    f\"{city}, {province.lower()}\",\n",
    "                    f\"{city} {province.lower()}\",\n",
    "                    f\"{city}, {province}\".lower(),\n",
    "                    f\"{city} {province}\".lower(),\n",
    "                    city  # Try just the city name\n",
    "                ]\n",
    "                \n",
    "                for combo in combinations:\n",
    "                    if combo in canadian_locations:\n",
    "                        print(f\"   ✅ Found PRIORITY 1 match: '{combo}' -> {canadian_locations[combo]}\")\n",
    "                        return canadian_locations[combo]\n",
    "    \n",
    "    # PRIORITY 2: Look for emoji + location patterns  \n",
    "    emoji_patterns = [\n",
    "        r'[📍🇨🇦🗺️🏠🏢]\\s*([^🔗\\n\\|]+?)(?:\\s*🔗|\\s*$|\\n|\\|)',\n",
    "        r'[\\u2600-\\u26FF\\u2700-\\u27BF\\u1F300-\\u1F5FF\\u1F600-\\u1F64F\\u1F680-\\u1F6FF]\\s*([a-zA-Z\\s,]+)',  # Extended emoji ranges including 📍\n",
    "    ]\n",
    "    \n",
    "    for pattern in emoji_patterns:\n",
    "        matches = re.findall(pattern, combined_text, re.IGNORECASE | re.MULTILINE)\n",
    "        for match in matches:\n",
    "            location_text = normalize_text_for_location(match).lower().strip()\n",
    "            \n",
    "            # Try the location text and also extract just city names\n",
    "            test_locations = [location_text]\n",
    "            \n",
    "            # Extract potential city/province combinations\n",
    "            words = location_text.split()\n",
    "            if len(words) >= 2:\n",
    "                # Try last two words (might be \"city province\")\n",
    "                test_locations.append(\" \".join(words[-2:]))\n",
    "                # Try first two words  \n",
    "                test_locations.append(\" \".join(words[:2]))\n",
    "                # Try each word individually\n",
    "                test_locations.extend(words)\n",
    "            \n",
    "            for test_loc in test_locations:\n",
    "                test_loc = test_loc.strip()\n",
    "                if len(test_loc) >= 3 and test_loc in canadian_locations:\n",
    "                    print(f\"   Found PRIORITY 2 match: '{test_loc}' -> {canadian_locations[test_loc]}\")\n",
    "                    return canadian_locations[test_loc]\n",
    "    \n",
    "    # PRIORITY 3: Look for contextual phrases\n",
    "    contextual_patterns = [\n",
    "        r'(?:based\\s+in|located\\s+in|serving|available\\s+in|from)\\s+([a-zA-Z\\s,]+?)(?:\\s|$|[.!])',\n",
    "        r'(?:in|at)\\s+([a-zA-Z\\s,]+?)(?:\\s|$|[.!])'\n",
    "    ]\n",
    "    \n",
    "    for pattern in contextual_patterns:\n",
    "        matches = re.findall(pattern, search_text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            location_text = match.strip()\n",
    "            \n",
    "            # Try the full match and individual words\n",
    "            test_locations = [location_text]\n",
    "            words = location_text.split()\n",
    "            test_locations.extend(words)\n",
    "            \n",
    "            for test_loc in test_locations:\n",
    "                test_loc = test_loc.strip()\n",
    "                if len(test_loc) >= 4 and test_loc in canadian_locations:\n",
    "                    print(f\"   Found PRIORITY 3 match: '{test_loc}' -> {canadian_locations[test_loc]}\")\n",
    "                    return canadian_locations[test_loc]\n",
    "    \n",
    "    # PRIORITY 4: Direct word matching in the search text\n",
    "    # Sort by length (longest first) to catch compound names like \"North Vancouver\"\n",
    "    sorted_locations = sorted(canadian_locations.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    for location_key, location_value in sorted_locations:\n",
    "        if len(location_key) >= 3:\n",
    "            # Use word boundary matching for better accuracy\n",
    "            pattern = r'\\b' + re.escape(location_key) + r'\\b'\n",
    "            if re.search(pattern, search_text, re.IGNORECASE):\n",
    "                print(f\"   Found PRIORITY 4 match: '{location_key}' -> {location_value}\")\n",
    "                return location_value\n",
    "    \n",
    "    print(f\"   No location found for {username or 'unknown'}\")\n",
    "    return None\n",
    "\n",
    "# --- Proxy Setup ---\n",
    "proxy_host = \"evo-pro.porterproxies.com\"\n",
    "proxy_port = 61236\n",
    "proxy_user = \"PP_F8AR2T6V9E-country-CA-session-2uil00vALK6T\"\n",
    "proxy_pass = \"663bei24\"\n",
    "\n",
    "def create_proxy_auth_extension(host, port, user, password):\n",
    "    manifest = \"\"\"\n",
    "    {\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"manifest_version\": 2,\n",
    "        \"name\": \"ProxyAuthExtension\",\n",
    "        \"permissions\": [\n",
    "            \"proxy\", \"tabs\", \"unlimitedStorage\", \"storage\", \"<all_urls>\", \"webRequest\", \"webRequestBlocking\"\n",
    "        ],\n",
    "        \"background\": { \"scripts\": [\"background.js\"] }\n",
    "    }\n",
    "    \"\"\"\n",
    "    background = f\"\"\"\n",
    "    chrome.proxy.settings.set({{\n",
    "        value: {{\n",
    "            mode: \"fixed_servers\",\n",
    "            rules: {{\n",
    "                singleProxy: {{\n",
    "                    scheme: \"http\",\n",
    "                    host: \"{host}\",\n",
    "                    port: parseInt({port})\n",
    "                }},\n",
    "                bypassList: [\"localhost\"]\n",
    "            }}\n",
    "        }},\n",
    "        scope: \"regular\"\n",
    "    }}, function() {{}});\n",
    "\n",
    "    chrome.webRequest.onAuthRequired.addListener(\n",
    "        function(details) {{\n",
    "            return {{\n",
    "                authCredentials: {{\n",
    "                    username: \"{user}\",\n",
    "                    password: \"{password}\"\n",
    "                }}\n",
    "            }};\n",
    "        }},\n",
    "        {{urls: [\"<all_urls>\"]}},\n",
    "        [\"blocking\"]\n",
    "    );\n",
    "    \"\"\"\n",
    "    plugin_file = 'proxy_auth_plugin.zip'\n",
    "    with zipfile.ZipFile(plugin_file, 'w') as zipf:\n",
    "        zipf.writestr(\"manifest.json\", manifest)\n",
    "        zipf.writestr(\"background.js\", background)\n",
    "    return plugin_file\n",
    "\n",
    "def safe_find_element(driver, by, value, wait_time=5):\n",
    "    try:\n",
    "        element = WebDriverWait(driver, wait_time).until(\n",
    "            EC.presence_of_element_located((by, value))\n",
    "        )\n",
    "        return element\n",
    "    except (TimeoutException, NoSuchElementException):\n",
    "        return None\n",
    "\n",
    "def safe_find_elements(driver, by, value, wait_time=5):\n",
    "    try:\n",
    "        WebDriverWait(driver, wait_time).until(\n",
    "            EC.presence_of_element_located((by, value))\n",
    "        )\n",
    "        elements = driver.find_elements(by, value)\n",
    "        return elements\n",
    "    except (TimeoutException, NoSuchElementException):\n",
    "        return []\n",
    "\n",
    "def extract_exact_links(text, html):\n",
    "    \"\"\"Extract any legitimate links from the provided text or HTML with comprehensive patterns\"\"\"\n",
    "    links = []\n",
    "    blacklisted_domains = [\n",
    "        'instagram.com', 'facebook.com', 'meta.com', 'threads.com', 'meta.ai',\n",
    "        'about.instagram.com', 'help.instagram.com', 'developers.facebook.com',\n",
    "        'android.com', 'apple.com', 'google.com', 'youtube.com', 'twitter.com'\n",
    "    ]\n",
    "    \n",
    "    # COMPREHENSIVE LINK PATTERNS - More patterns for better coverage\n",
    "    link_patterns = [\n",
    "        # Booking platforms and appointment links - specific patterns first\n",
    "        r'(https?://[^/\\s]+\\.setmore\\.com/[a-zA-Z0-9_\\-/]+)',  # Complete Setmore URLs\n",
    "        r'([^/\\s]+\\.setmore\\.com/[a-zA-Z0-9_\\-/]+)',  # Setmore URLs without protocol\n",
    "        r'(https?://book\\.squareup\\.com/appointments/[a-zA-Z0-9_\\-/]+(?:\\?[^\\s]+)?)',  # Complete Square URLs with query params\n",
    "        r'(book\\.squareup\\.com/appointments/[a-zA-Z0-9_\\-/]+(?:\\?[^\\s]+)?)',  # Square URLs without protocol\n",
    "        r'(https?://squareup\\.com/appointments/[a-zA-Z0-9_\\-/]+(?:\\?[^\\s]+)?)',  # Another Square format\n",
    "        r'(squareup\\.com/appointments/[a-zA-Z0-9_\\-/]+(?:\\?[^\\s]+)?)',  # Square without protocol\n",
    "        r'(https?://[^/\\s]+\\.acuityscheduling\\.com/[a-zA-Z0-9_\\-/]+)',  # Acuity Scheduling\n",
    "        r'([^/\\s]+\\.acuityscheduling\\.com/[a-zA-Z0-9_\\-/]+)',  # Acuity without protocol\n",
    "        r'(https?://[^/\\s]+\\.simplybook\\.it/[a-zA-Z0-9_\\-/]+)',  # SimplyBook.it\n",
    "        r'([^/\\s]+\\.simplybook\\.it/[a-zA-Z0-9_\\-/]+)',  # SimplyBook.it without protocol\n",
    "        r'(https?://[^/\\s]+\\.vagaro\\.com/[a-zA-Z0-9_\\-/]+)',  # Vagaro\n",
    "        r'([^/\\s]+\\.vagaro\\.com/[a-zA-Z0-9_\\-/]+)',  # Vagaro without protocol\n",
    "        r'(https?://[^/\\s]+\\.schedulicity\\.com/[a-zA-Z0-9_\\-/]+)',  # Schedulicity\n",
    "        r'([^/\\s]+\\.schedulicity\\.com/[a-zA-Z0-9_\\-/]+)',  # Schedulicity without protocol\n",
    "        r'(https?://[^/\\s]+\\.appointy\\.com/[a-zA-Z0-9_\\-/]+)',  # Appointy\n",
    "        r'([^/\\s]+\\.appointy\\.com/[a-zA-Z0-9_\\-/]+)',  # Appointy without protocol\n",
    "        r'(https?://[^/\\s]+\\.booksy\\.com/[a-zA-Z0-9_\\-/]+)',  # Booksy\n",
    "        r'([^/\\s]+\\.booksy\\.com/[a-zA-Z0-9_\\-/]+)',  # Booksy without protocol\n",
    "        r'(https?://[^/\\s]+\\.fresha\\.com/[a-zA-Z0-9_\\-/]+)',  # Fresha\n",
    "        r'([^/\\s]+\\.fresha\\.com/[a-zA-Z0-9_\\-/]+)',  # Fresha without protocol\n",
    "        r'(https?://[^/\\s]+\\.genbook\\.com/[a-zA-Z0-9_\\-/]+)',  # Genbook\n",
    "        r'([^/\\s]+\\.genbook\\.com/[a-zA-Z0-9_\\-/]+)',  # Genbook without protocol\n",
    "        r'(https?://[^/\\s]+\\.timely\\.com/[a-zA-Z0-9_\\-/]+)',  # Timely\n",
    "        r'([^/\\s]+\\.timely\\.com/[a-zA-Z0-9_\\-/]+)',  # Timely without protocol\n",
    "        r'(https?://[^/\\s]+\\.planity\\.com/[a-zA-Z0-9_\\-/]+)',  # Planity\n",
    "        r'([^/\\s]+\\.planity\\.com/[a-zA-Z0-9_\\-/]+)',  # Planity without protocol\n",
    "        \n",
    "        # Forms and Google services\n",
    "        r'(forms\\.gle/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(docs\\.google\\.com/forms/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(https?://forms\\.gle/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://docs\\.google\\.com/forms/[a-zA-Z0-9_\\-/]+)',\n",
    "        \n",
    "        # Square and other payment links\n",
    "        r'(https?://square\\.site/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(square\\.site/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'([a-zA-Z0-9_\\-]+\\.square\\.site)',\n",
    "        r'(https?://[a-zA-Z0-9_\\-]+\\.square\\.site)',\n",
    "        \n",
    "        # WhatsApp and messaging links\n",
    "        r'(wa\\.me/message/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(wa\\.me/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(https?://wa\\.me/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(api\\.whatsapp\\.com/send\\?phone=[0-9]+)',\n",
    "        r'(https?://api\\.whatsapp\\.com/send\\?phone=[0-9]+)',\n",
    "        \n",
    "        # Calendar and scheduling services\n",
    "        r'(calendly\\.com/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(https?://calendly\\.com/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(acuity\\w*\\.com/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(https?://acuity\\w*\\.com/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(booking\\.page/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(https?://booking\\.page/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(book\\.[a-zA-Z0-9_\\-]+\\.[a-z]{2,}/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(https?://book\\.[a-zA-Z0-9_\\-]+\\.[a-z]{2,}/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(schedule\\.[a-zA-Z0-9_\\-]+\\.[a-z]{2,}/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(https?://schedule\\.[a-zA-Z0-9_\\-]+\\.[a-z]{2,}/[a-zA-Z0-9_\\-/]+)',\n",
    "        \n",
    "        # Link in bio services - comprehensive list\n",
    "        r'(linktr\\.ee/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://linktr\\.ee/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(bio\\.site/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://bio\\.site/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(linkin\\.bio/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://linkin\\.bio/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(beacons\\.ai/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://beacons\\.ai/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(campsite\\.bio/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://campsite\\.bio/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(solo\\.to/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://solo\\.to/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(linkpop\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://linkpop\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(lnk\\.bio/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://lnk\\.bio/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(tap\\.bio/@[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://tap\\.bio/@[a-zA-Z0-9_\\-]+)',\n",
    "        r'(flow\\.page/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://flow\\.page/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(milkshake\\.app/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://milkshake\\.app/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(heylink\\.me/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://heylink\\.me/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(direct\\.me/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://direct\\.me/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(withkoji\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://withkoji\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(carrd\\.co/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://carrd\\.co/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(taplink\\.cc/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://taplink\\.cc/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(msha\\.ke/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://msha\\.ke/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(shorby\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://shorby\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(shor\\.by/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://shor\\.by/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(allmylinks\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://allmylinks\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(onebio\\.link/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://onebio\\.link/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(about\\.me/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://about\\.me/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(creator\\.link/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://creator\\.link/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(linkby\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://linkby\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(lnk\\.bio/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://lnk\\.bio/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(linkr\\.bio/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://linkr\\.bio/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(stan\\.store/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://stan\\.store/[a-zA-Z0-9_\\-]+)',\n",
    "        \n",
    "        # URL shorteners\n",
    "        r'(bit\\.ly/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://bit\\.ly/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(tinyurl\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://tinyurl\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(t\\.co/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://t\\.co/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(goo\\.gl/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://goo\\.gl/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(ow\\.ly/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://ow\\.ly/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(buff\\.ly/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://buff\\.ly/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(cutt\\.ly/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://cutt\\.ly/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(rebrand\\.ly/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://rebrand\\.ly/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(short\\.link/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://short\\.link/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(s\\.id/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://s\\.id/[a-zA-Z0-9_\\-]+)',\n",
    "        \n",
    "        # E-commerce and store links\n",
    "        r'(https?://[a-zA-Z0-9_\\-]+\\.myshopify\\.com)',\n",
    "        r'([a-zA-Z0-9_\\-]+\\.myshopify\\.com)',\n",
    "        r'(https?://[a-zA-Z0-9_\\-]+\\.etsy\\.com)',\n",
    "        r'([a-zA-Z0-9_\\-]+\\.etsy\\.com)',\n",
    "        r'(https?://[a-zA-Z0-9_\\-]+\\.bigcommerce\\.com)',\n",
    "        r'([a-zA-Z0-9_\\-]+\\.bigcommerce\\.com)',\n",
    "        r'(https?://[a-zA-Z0-9_\\-]+\\.square\\.site)',\n",
    "        r'([a-zA-Z0-9_\\-]+\\.square\\.site)',\n",
    "        \n",
    "        # Social media scheduling and other services\n",
    "        r'(later\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://later\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(buffer\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://buffer\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(hootsuite\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        r'(https?://hootsuite\\.com/[a-zA-Z0-9_\\-]+)',\n",
    "        \n",
    "        # Website builders and portfolios\n",
    "        r'(https?://[a-zA-Z0-9_\\-]+\\.wixsite\\.com)',\n",
    "        r'([a-zA-Z0-9_\\-]+\\.wixsite\\.com)',\n",
    "        r'(https?://[a-zA-Z0-9_\\-]+\\.weebly\\.com)',\n",
    "        r'([a-zA-Z0-9_\\-]+\\.weebly\\.com)',\n",
    "        r'(https?://[a-zA-Z0-9_\\-]+\\.squarespace\\.com)',\n",
    "        r'([a-zA-Z0-9_\\-]+\\.squarespace\\.com)',\n",
    "        r'(https?://[a-zA-Z0-9_\\-]+\\.wordpress\\.com)',\n",
    "        r'([a-zA-Z0-9_\\-]+\\.wordpress\\.com)',\n",
    "        r'(https?://[a-zA-Z0-9_\\-]+\\.godaddysites\\.com)',\n",
    "        r'([a-zA-Z0-9_\\-]+\\.godaddysites\\.com)',\n",
    "        \n",
    "        # Email marketing and newsletters\n",
    "        r'(mailchi\\.mp/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(https?://mailchi\\.mp/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(us\\d+\\.campaign-archive\\.com/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(https?://us\\d+\\.campaign-archive\\.com/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(constantcontact\\.com/[a-zA-Z0-9_\\-/]+)',\n",
    "        r'(https?://constantcontact\\.com/[a-zA-Z0-9_\\-/]+)',\n",
    "        \n",
    "        # Regular websites - broader coverage\n",
    "        r'(https?://[a-zA-Z0-9_.\\-]+\\.[a-z]{2,}(?:/[^\"\\'<>\\s]*)?(?:\\?[^\\s\"\\'<>]+)?)',  # With query params\n",
    "        r'(www\\.[a-zA-Z0-9_.\\-]+\\.[a-z]{2,}(?:/[^\"\\'<>\\s]*)?(?:\\?[^\\s\"\\'<>]+)?)',  # With query params\n",
    "        r'([a-zA-Z0-9_\\-]+\\.(com|ca|net|org|studio|beauty|store|io|gl|gle|me|co|us|biz|shop|info|xyz|online|site|website|app|blog|health|spa|salon|clinic|services|boutique)(?:/[^\"\\'<>\\s]*)?(?:\\?[^\\s\"\\'<>]+)?)'  # Extended TLDs\n",
    "    ]\n",
    "    \n",
    "    # First, search for explicit links in the visible text\n",
    "    for pattern in link_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            # Handle tuple results from regex groups\n",
    "            if isinstance(match, tuple):\n",
    "                match = match[0]\n",
    "                \n",
    "            # Skip blacklisted domains\n",
    "            if any(excluded in match.lower() for excluded in blacklisted_domains):\n",
    "                continue\n",
    "                \n",
    "            # Format the link properly\n",
    "            if match.startswith(('http://', 'https://')):\n",
    "                links.append(match)\n",
    "            elif match.startswith('www.'):\n",
    "                links.append(f\"https://{match}\")\n",
    "            elif any(match.startswith(prefix) for prefix in [\n",
    "                'forms.gle', 'wa.me', 'bit.ly', 'linktr.ee', 'bio.site', 'calendly.com',\n",
    "                'book.squareup.com', 'square.site', 'setmore.com', 'booksy.com'\n",
    "            ]):\n",
    "                links.append(f\"https://{match}\")\n",
    "            elif '.setmore.com' in match:\n",
    "                # Special handling for setmore links to ensure correct format\n",
    "                if not match.startswith(('http://', 'https://')):\n",
    "                    links.append(f\"https://{match}\")\n",
    "                else:\n",
    "                    links.append(match)\n",
    "            elif any(booking_service in match.lower() for booking_service in [\n",
    "                'book.squareup.com', 'squareup.com/appointments', 'acuityscheduling.com',\n",
    "                'simplybook.it', 'vagaro.com', 'schedulicity.com', 'appointy.com'\n",
    "            ]):\n",
    "                # Special handling for booking service links to ensure query parameters are preserved\n",
    "                if not match.startswith(('http://', 'https://')):\n",
    "                    links.append(f\"https://{match}\")\n",
    "                else:\n",
    "                    links.append(match)\n",
    "            else:\n",
    "                links.append(f\"https://{match}\")\n",
    "    \n",
    "    # Next, extract URLs from href attributes in the HTML\n",
    "    # Look specifically for the link elements that are likely to be prominent in profiles\n",
    "    href_pattern = r'<a[^>]*href=\"(https?://[^\"]+(?:\\?[^\"]+)?)\"[^>]*>'\n",
    "    href_matches = re.findall(href_pattern, html, re.IGNORECASE)\n",
    "    \n",
    "    for url in href_matches:\n",
    "        # Skip blacklisted domains\n",
    "        if any(excluded in url.lower() for excluded in blacklisted_domains):\n",
    "            continue\n",
    "            \n",
    "        # Only add if not already in the list\n",
    "        if url not in links:\n",
    "            links.append(url)\n",
    "    \n",
    "    # Look for specific links related to scheduling/booking in the HTML\n",
    "    booking_patterns = [\n",
    "        r'href=\"(https?://[^\"]*?book[^\"]*?)\"',\n",
    "        r'href=\"(https?://[^\"]*?appointment[^\"]*?)\"',\n",
    "        r'href=\"(https?://[^\"]*?schedule[^\"]*?)\"',\n",
    "        r'href=\"(https?://[^\"]*?booking[^\"]*?)\"',\n",
    "        r'href=\"(https?://[^\"]*?reserve[^\"]*?)\"',\n",
    "        r'href=\"(https?://[^\"]*?setmore[^\"]*?)\"',\n",
    "        r'href=\"(https?://[^\"]*?squareup[^\"]*?)\"',\n",
    "        r'href=\"(https?://[^\"]*?acuity[^\"]*?)\"',\n",
    "        r'href=\"(https?://[^\"]*?calendly[^\"]*?)\"',\n",
    "        r'href=\"(https?://[^\"]*?booksy[^\"]*?)\"',\n",
    "        r'href=\"(https?://[^\"]*?fresha[^\"]*?)\"',\n",
    "        r'href=\"(https?://[^\"]*?vagaro[^\"]*?)\"'\n",
    "    ]\n",
    "    \n",
    "    for pattern in booking_patterns:\n",
    "        booking_links = re.findall(pattern, html, re.IGNORECASE)\n",
    "        for url in booking_links:\n",
    "            if not any(excluded in url.lower() for excluded in blacklisted_domains) and url not in links:\n",
    "                links.append(url)\n",
    "    \n",
    "    # Prioritize the links based on type (booking links are highest priority)\n",
    "    prioritized_links = []\n",
    "    booking_links = []\n",
    "    linkinbio_links = []\n",
    "    other_links = []\n",
    "    \n",
    "    # Sort links into categories\n",
    "    for link in links:\n",
    "        link_lower = link.lower()\n",
    "        if any(booking_term in link_lower for booking_term in [\n",
    "            'setmore.com', 'book.squareup.com', 'squareup.com/appointments', \n",
    "            'acuityscheduling.com', 'simplybook.it', 'vagaro.com', 'schedulicity.com',\n",
    "            'appointy.com', 'booksy.com', 'fresha.com', 'genbook.com', 'timely.com',\n",
    "            'planity.com', 'calendly.com', 'book', 'appointment', 'schedule', 'booking', 'reserve'\n",
    "        ]):\n",
    "            booking_links.append(link)\n",
    "        elif any(linkinbio_term in link_lower for linkinbio_term in [\n",
    "            'linktr.ee', 'bio.site', 'linkin.bio', 'beacons.ai', 'campsite.bio',\n",
    "            'solo.to', 'linkpop.com', 'lnk.bio', 'tap.bio', 'flow.page',\n",
    "            'milkshake.app', 'heylink.me', 'direct.me', 'carrd.co', 'taplink.cc'\n",
    "        ]):\n",
    "            linkinbio_links.append(link)\n",
    "        else:\n",
    "            other_links.append(link)\n",
    "    \n",
    "    # Combine links in priority order\n",
    "    prioritized_links.extend(booking_links)\n",
    "    prioritized_links.extend(linkinbio_links)\n",
    "    prioritized_links.extend(other_links)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_links = []\n",
    "    seen = set()\n",
    "    for link in prioritized_links:\n",
    "        # Clean the URL - but preserve query parameters for booking links\n",
    "        if any(booking_term in link.lower() for booking_term in [\n",
    "            'setmore.com', 'book.squareup.com', 'squareup.com/appointments',\n",
    "            'calendly.com', 'acuityscheduling.com', 'forms.gle'\n",
    "        ]):\n",
    "            # For booking links, keep query parameters but normalize the rest\n",
    "            clean_link = link.rstrip('/')\n",
    "        else:\n",
    "            # For other links, strip trailing slashes and query params\n",
    "            clean_link = re.sub(r'\\?.*$', '', link).rstrip('/')\n",
    "            \n",
    "        if clean_link not in seen:\n",
    "            seen.add(clean_link)\n",
    "            unique_links.append(link)\n",
    "    \n",
    "    return unique_links\n",
    "\n",
    "def get_all_page_text(driver):\n",
    "    \"\"\"Extract all visible text from the page\"\"\"\n",
    "    all_text_parts = []\n",
    "    \n",
    "    try:\n",
    "        # Get body text\n",
    "        body_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "        if body_text:\n",
    "            all_text_parts.append(body_text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Get header text (often contains bio)\n",
    "        header = driver.find_element(By.TAG_NAME, \"header\")\n",
    "        if header and header.text:\n",
    "            all_text_parts.append(header.text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return \" \".join(all_text_parts)\n",
    "\n",
    "def scrape_profile(driver, username):\n",
    "    \"\"\"Scrape a single Instagram profile and return its data\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    driver.get(f\"https://www.instagram.com/{username}/\")\n",
    "    \n",
    "    # Smart loading - check if page loaded faster than expected\n",
    "    time.sleep(2)  # Minimum wait\n",
    "    \n",
    "    # Check if we can find critical elements early\n",
    "    quick_check = safe_find_element(driver, By.TAG_NAME, \"header\", wait_time=1)\n",
    "    if not quick_check:\n",
    "        # If page still loading, wait a bit more\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "    \n",
    "    # Initialize variables\n",
    "    bio = \"\"\n",
    "    website = None\n",
    "    followers = 0\n",
    "    display_name = \"\"\n",
    "    all_text = \"\"\n",
    "    \n",
    "    # Get page source\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Try different methods to get the text content\n",
    "    try:\n",
    "        all_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "    except:\n",
    "        all_text = \"\"\n",
    "    \n",
    "    # Extract display name with multiple methods\n",
    "    h2_elements = safe_find_elements(driver, By.TAG_NAME, \"h2\")\n",
    "    for elem in h2_elements:\n",
    "        if elem.text and len(elem.text) > 0:\n",
    "            display_name = elem.text\n",
    "            break\n",
    "    \n",
    "    if not display_name:\n",
    "        h1_elements = safe_find_elements(driver, By.TAG_NAME, \"h1\")\n",
    "        for elem in h1_elements:\n",
    "            if elem.text and len(elem.text) > 0:\n",
    "                display_name = elem.text\n",
    "                break\n",
    "    \n",
    "    # Extract follower count using multiple approaches\n",
    "    follower_text = None\n",
    "    \n",
    "    # Method 1: Look for elements with follower counts using comprehensive selectors\n",
    "    follower_selectors = [\n",
    "        # Modern Instagram selectors (2024/2025)\n",
    "        \"//span[contains(@class, '_ac2a')]/span\",\n",
    "        \"//span[contains(@class, '_ac2a')]\",\n",
    "        \"//a[contains(@href, '/followers/')]/span\",\n",
    "        \"//a[contains(@href, '/followers/')]\",\n",
    "        \"//span[contains(text(), 'followers')]/preceding-sibling::*\",\n",
    "        \"//div[contains(text(), 'followers')]/preceding-sibling::div\",\n",
    "        \"//div[contains(text(), 'followers')]/parent::div/div[1]\",\n",
    "        \n",
    "        # Accessibility attributes\n",
    "        \"//span[contains(@title, 'followers')]\",\n",
    "        \"//span[contains(@aria-label, 'followers')]\", \n",
    "        \"//a[contains(@aria-label, 'followers')]\",\n",
    "        \n",
    "        # Generic patterns\n",
    "        \"//span[contains(@class, 'follower')]\",\n",
    "        \"//a[contains(@href, 'followers')]//span\",\n",
    "        \"//div[contains(@class, 'follower')]//span\",\n",
    "        \n",
    "        # More flexible patterns\n",
    "        \"//span[text()[contains(., 'followers')]]/preceding-sibling::span\",\n",
    "        \"//span[text()[contains(., 'followers')]]/parent::*/span[1]\",\n",
    "        \"//div[text()[contains(., 'followers')]]/preceding-sibling::span\",\n",
    "        \n",
    "        # Catch-all for any span with numbers near \"followers\"  \n",
    "        \"//span[following-sibling::*[contains(text(), 'followers')]]\",\n",
    "        \"//span[preceding-sibling::*[contains(text(), 'followers')]]\"\n",
    "    ]\n",
    "    \n",
    "    for selector in follower_selectors:\n",
    "        elements = safe_find_elements(driver, By.XPATH, selector)\n",
    "        for elem in elements:\n",
    "            try:\n",
    "                elem_text = elem.get_attribute(\"title\") or elem.get_attribute(\"aria-label\") or elem.text\n",
    "                if elem_text and any(c.isdigit() for c in elem_text):\n",
    "                    follower_text = elem_text\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        if follower_text:\n",
    "            break\n",
    "    \n",
    "    # Method 2: Try to find the followers pattern in page text with comprehensive regex\n",
    "    if not follower_text:\n",
    "        follower_patterns = [\n",
    "            # Standard patterns\n",
    "            r'(\\d+(?:[,\\.]\\d+)*[KkMm]?)\\s*followers',\n",
    "            r'(\\d+(?:[,\\.]\\d+)*[KkMm]?)\\s*Followers',\n",
    "            r'(\\d+(?:[,\\.]\\d+)*[KkMm]?)\\s*FOLLOWERS',\n",
    "            \n",
    "            # With different spacing\n",
    "            r'(\\d+(?:[,\\.\\s]\\d+)*[KkMm]?)\\s*followers', \n",
    "            r'(\\d+(?:[,\\.\\s]\\d+)*[KkMm]?)\\s*Followers',\n",
    "            \n",
    "            # Decimal variations\n",
    "            r'(\\d+[.,]\\d+[KkMm])\\s*followers',\n",
    "            r'(\\d+[.,]\\d+[KkMm])\\s*Followers',\n",
    "            \n",
    "            # Just numbers near followers\n",
    "            r'(\\d{1,3}(?:[,\\.]\\d{3})*)\\s*followers',\n",
    "            r'(\\d+)\\s*[Kk]\\s*followers',\n",
    "            r'(\\d+[.,]\\d+)\\s*[KkMm]\\s*followers',\n",
    "            \n",
    "            # Alternative formats\n",
    "            r'followers[:\\s]*(\\d+(?:[,\\.]\\d+)*[KkMm]?)',\n",
    "            r'Followers[:\\s]*(\\d+(?:[,\\.]\\d+)*[KkMm]?)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in follower_patterns:\n",
    "            follower_pattern = re.search(pattern, all_text)\n",
    "            if follower_pattern:\n",
    "                follower_text = follower_pattern.group(1)\n",
    "                break\n",
    "    \n",
    "    # Convert follower text to number - FIXED DECIMAL HANDLING\n",
    "    if follower_text:\n",
    "        # Clean but preserve decimal for K/M conversion\n",
    "        clean = follower_text.replace(\",\", \"\").replace(\" \", \"\").lower()\n",
    "        \n",
    "        try:\n",
    "            if 'k' in clean:\n",
    "                # Handle decimal in K values (e.g., \"4.3K\" = 4300)\n",
    "                num_part = clean.replace(\"k\", \"\").replace(\"followers\", \"\").strip()\n",
    "                followers = int(float(num_part) * 1000)\n",
    "            elif 'm' in clean:\n",
    "                # Handle decimal in M values (e.g., \"1.2M\" = 1200000) \n",
    "                num_part = clean.replace(\"m\", \"\").replace(\"followers\", \"\").strip()\n",
    "                followers = int(float(num_part) * 1_000_000)\n",
    "            else:\n",
    "                # For regular numbers, remove all non-digits after cleaning\n",
    "                clean_no_decimals = clean.replace(\".\", \"\").replace(\"followers\", \"\")\n",
    "                digits = ''.join(filter(str.isdigit, clean_no_decimals))\n",
    "                if digits:\n",
    "                    followers = int(digits)\n",
    "        except (ValueError, TypeError):\n",
    "            followers = 0\n",
    "    \n",
    "    # Extract bio content using multiple approaches\n",
    "    bio_text_parts = []\n",
    "    \n",
    "    # Method 1: Look for spans with _ap3a class (Instagram's bio container)\n",
    "    bio_spans = safe_find_elements(driver, By.XPATH, \"//span[contains(@class, '_ap3a')]\")\n",
    "    if bio_spans:\n",
    "        for span in bio_spans:\n",
    "            span_text = span.text.strip()\n",
    "            if span_text and \"Follow\" not in span_text and \"Message\" not in span_text:\n",
    "                bio_text_parts.append(span_text)\n",
    "    \n",
    "    # Method 2: Look for divs that might contain bio text\n",
    "    if not bio_text_parts:\n",
    "        bio_class_selectors = ['x7a106', 'x1lliihq', 'xat24cr', 'x1emribx']\n",
    "        for class_part in bio_class_selectors:\n",
    "            bio_divs = safe_find_elements(driver, By.XPATH, f\"//div[contains(@class, '{class_part}')]\")\n",
    "            for div in bio_divs:\n",
    "                div_text = div.text.strip()\n",
    "                if div_text and len(div_text) > 10:  # Avoid very short segments\n",
    "                    bio_text_parts.append(div_text)\n",
    "                    break\n",
    "            if bio_text_parts:\n",
    "                break\n",
    "    \n",
    "    # Method 3: Try to extract from header section\n",
    "    if not bio_text_parts:\n",
    "        header = safe_find_element(driver, By.XPATH, \"//header[@role='banner']\") or safe_find_element(driver, By.TAG_NAME, \"header\")\n",
    "        if header:\n",
    "            header_text = header.text\n",
    "            # Filter out UI elements\n",
    "            ui_elements = [\"posts\", \"followers\", \"following\", \"Follow\", \"Message\", \"Edit profile\"]\n",
    "            lines = [line for line in header_text.split('\\n') if line and not any(ui in line for ui in ui_elements)]\n",
    "            if lines:\n",
    "                # Usually the first line is the username, so skip it\n",
    "                if len(lines) > 1:\n",
    "                    bio_text_parts = lines[1:]\n",
    "                else:\n",
    "                    bio_text_parts = lines\n",
    "    \n",
    "    # Join bio parts and clean up\n",
    "    if bio_text_parts:\n",
    "        bio = \"\\n\".join(bio_text_parts)\n",
    "        # Remove common UI text\n",
    "        bio = re.sub(r'(Follow|Message|Edit profile|Switch|More actions|Posts|Reels|Tagged)', '', bio, flags=re.IGNORECASE)\n",
    "        bio = re.sub(r'\\s+', ' ', bio).strip()\n",
    "    \n",
    "    # If still empty, try extracting from HTML using regex\n",
    "    if not bio:\n",
    "        bio_html_patterns = [\n",
    "            r'<div[^>]*class=\"[^\"]*x7a106[^\"]*\"[^>]*>(.*?)</div>',\n",
    "            r'<span[^>]*class=\"[^\"]*_ap3a[^\"]*\"[^>]*>(.*?)</span>',\n",
    "            r'<div[^>]*class=\"[^\"]*x1lliihq[^\"]*\"[^>]*>(.*?)</div>'\n",
    "        ]\n",
    "        \n",
    "        for pattern in bio_html_patterns:\n",
    "            bio_matches = re.findall(pattern, page_source, re.DOTALL)\n",
    "            if bio_matches:\n",
    "                for match in bio_matches:\n",
    "                    text = re.sub(r'<[^>]*>', ' ', match).strip()\n",
    "                    if text and len(text) > 10:\n",
    "                        bio = text\n",
    "                        break\n",
    "                if bio:\n",
    "                    break\n",
    "    \n",
    "    # Extract links from the profile\n",
    "    all_links = extract_exact_links(bio + ' ' + all_text, page_source)\n",
    "    \n",
    "    # Filter out any blacklisted domain links\n",
    "    filtered_links = [\n",
    "        link for link in all_links \n",
    "        if not any(blacklisted in link.lower() for blacklisted in [\n",
    "            \"meta.ai\", \"instagram.com\", \"facebook.com\", \"meta.com\", \"threads.com\"\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    # Set website to the first valid link (if any)\n",
    "    website = None if not filtered_links else filtered_links[0]\n",
    "    \n",
    "    # If there are links in the bio, try to get them directly\n",
    "    if not website and bio:\n",
    "        # Check for specific patterns that indicate a booking/website link\n",
    "        for signal in [\"book\", \"link\", \"website\", \"site\", \"👇\", \"⬇️\", \"📲\", \"book below\", \"linkin.bio\"]:\n",
    "            if signal.lower() in bio.lower():\n",
    "                if filtered_links:\n",
    "                    website = filtered_links[0]\n",
    "                    break\n",
    "    \n",
    "    # Directly look for link elements in the profile that are displayed prominently\n",
    "    link_elements = safe_find_elements(driver, By.TAG_NAME, \"a\")\n",
    "    for link in link_elements:\n",
    "        try:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and not any(excluded in href.lower() for excluded in [\n",
    "                \"instagram.com\", \"facebook.com\", \"meta.com\", \"threads.com\", \"meta.ai\"\n",
    "            ]):\n",
    "                # If the link is visible and appears to be a primary link\n",
    "                if link.is_displayed():\n",
    "                    # Higher priority for booking links\n",
    "                    if any(term in href.lower() for term in [\n",
    "                        \"book\", \"appointment\", \"forms.gle\", \"square.site\", \"wa.me\", \"squareup.com\",\n",
    "                        \"linktr.ee\", \"bio.site\", \"setmore.com\", \"calendly.com\", \"booksy.com\",\n",
    "                        \"acuityscheduling.com\", \"fresha.com\", \"vagaro.com\"\n",
    "                    ]):\n",
    "                        website = href\n",
    "                        if href not in filtered_links:\n",
    "                            filtered_links.insert(0, href)\n",
    "                        break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return {\n",
    "        'username': username,\n",
    "        'display_name': display_name,\n",
    "        'bio': bio,\n",
    "        'followers': followers,\n",
    "        'website': website,\n",
    "        'all_links': filtered_links,\n",
    "        'all_text': all_text,\n",
    "        'scrape_time': time.time() - start_time  # Track how long each profile took\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # --- Connect to DB ---\n",
    "    conn = sqlite3.connect('instagram_leads.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    '''\n",
    "    # Clear old records\n",
    "    cursor.execute(\"DELETE FROM profiles\")\n",
    "    conn.commit()\n",
    "    print(\"Cleared existing profiles from the database.\")\n",
    "    '''\n",
    "    \n",
    "    # --- Load usernames ---\n",
    "    with open(\"/Users/saamsani/Desktop/usernames_raw_canada.json\") as f:\n",
    "         usernames = json.load(f)\n",
    "    print(f\"📋 Loaded {len(usernames)} usernames.\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # --- Setup Chrome with Proxy ---\n",
    "    options = Options()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    options.add_argument(\"--disable-popup-blocking\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    options.add_extension(create_proxy_auth_extension(proxy_host, proxy_port, proxy_user, proxy_pass))\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    try:\n",
    "        # --- Login First ---\n",
    "        driver.get(\"https://www.instagram.com/accounts/login/\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Handle cookies popup\n",
    "        cookie_buttons = safe_find_elements(driver, By.XPATH, \"//button[contains(text(), 'cookies') or contains(text(), 'Accept') or contains(text(), 'Allow')]\")\n",
    "        for button in cookie_buttons:\n",
    "            if button.is_displayed():\n",
    "                button.click()\n",
    "                break\n",
    "        \n",
    "        # Login\n",
    "        username_field = safe_find_element(driver, By.NAME, \"username\") or safe_find_element(driver, By.XPATH, \"//input[@name='username' or @aria-label='Phone number, username, or email']\")\n",
    "        password_field = safe_find_element(driver, By.NAME, \"password\") or safe_find_element(driver, By.XPATH, \"//input[@name='password' or @aria-label='Password']\")\n",
    "        \n",
    "        if username_field and password_field:\n",
    "            username_field.send_keys(\"leadtester101\")\n",
    "            password_field.send_keys(\"lash$101\")\n",
    "            \n",
    "            # Find and click submit button\n",
    "            submit_button = safe_find_element(driver, By.XPATH, \"//button[@type='submit']\") or \\\n",
    "                           safe_find_element(driver, By.XPATH, \"//div[text()='Log In']/ancestor::button\") or \\\n",
    "                           safe_find_element(driver, By.XPATH, \"//button[contains(text(), 'Log In')]\")\n",
    "                           \n",
    "            if submit_button:\n",
    "                submit_button.click()\n",
    "                print(\"Login submitted\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(\"Submit button not found\")\n",
    "        \n",
    "        # Handle post-login popups\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Handle \"Save Info\" popup\n",
    "        save_info_buttons = safe_find_elements(driver, By.XPATH, \"//button[contains(text(), 'Not Now') or contains(text(), 'Skip')]\")\n",
    "        for button in save_info_buttons:\n",
    "            if button.is_displayed():\n",
    "                button.click()\n",
    "                time.sleep(1)  # Reduced from 2 seconds\n",
    "                break\n",
    "        \n",
    "        # Handle notifications popup\n",
    "        notif_buttons = safe_find_elements(driver, By.XPATH, \"//button[contains(text(), 'Not Now') or contains(text(), 'Cancel')]\")\n",
    "        for button in notif_buttons:\n",
    "            if button.is_displayed():\n",
    "                button.click()\n",
    "                time.sleep(1)  # Reduced from 2 seconds  \n",
    "                break\n",
    "        \n",
    "        # --- Handle additional popups that might appear ---\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Handle \"Save Info\" popup again (sometimes appears twice)\n",
    "        save_info_buttons = safe_find_elements(driver, By.XPATH, \"//button[contains(text(), 'Not Now') or contains(text(), 'Skip')]\")\n",
    "        for button in save_info_buttons:\n",
    "            if button.is_displayed():\n",
    "                button.click()\n",
    "                time.sleep(2)\n",
    "                break\n",
    "        \n",
    "        # Handle notifications popup again (sometimes appears twice)\n",
    "        notif_buttons = safe_find_elements(driver, By.XPATH, \"//button[contains(text(), 'Not Now') or contains(text(), 'Cancel')]\")\n",
    "        for button in notif_buttons:\n",
    "            if button.is_displayed():\n",
    "                button.click()\n",
    "                time.sleep(2)\n",
    "                break\n",
    "        \n",
    "        # --- Batching setup ---\n",
    "        batch_size = 150  \n",
    "        batch_delay = 300  \n",
    "        \n",
    "        print(\"Starting profile enrichment...\")\n",
    "        \n",
    "        # Process usernames in batches\n",
    "        for i in range(0, len(usernames), batch_size):\n",
    "            batch = usernames[i:i + batch_size]\n",
    "            print(f\"\\n Processing batch {i // batch_size + 1} of {len(usernames) // batch_size + 1}...\")\n",
    "            \n",
    "            for username in batch:\n",
    "                try:\n",
    "                    print(f\" Scraping profile: {username}\")\n",
    "                    \n",
    "                    # Scrape profile data\n",
    "                    profile_data = scrape_profile(driver, username)\n",
    "                    \n",
    "                    # Extract fields\n",
    "                    bio = profile_data['bio']\n",
    "                    display_name = profile_data['display_name']\n",
    "                    followers = profile_data['followers']\n",
    "                    website = profile_data['website']\n",
    "                    all_links = profile_data['all_links']\n",
    "                    all_text = profile_data['all_text']\n",
    "                    \n",
    "                    # Process with enhanced functions for location and contact info\n",
    "                    email, phone, final_website = extract_contact_info(bio, website)\n",
    "                    location = enhanced_location_detection(bio, display_name, username, all_text)\n",
    "                    \n",
    "                    # Special case handling for known usernames and locations\n",
    "                    if username == \"dani.thenailwitch\" and \"sackville\" in bio.lower():\n",
    "                        location = \"Nova Scotia\"\n",
    "                    \n",
    "                    # Special case handling for booking links\n",
    "                    if username == \"thewinkstudiomtl\":\n",
    "                        # Look specifically for setmore links\n",
    "                        setmore_link = None\n",
    "                        for link in all_links:\n",
    "                            if \"setmore.com\" in link.lower():\n",
    "                                setmore_link = link\n",
    "                                break\n",
    "                        if setmore_link:\n",
    "                            final_website = setmore_link\n",
    "                    \n",
    "                    # Score the lead\n",
    "                    score = score_lead(bio, email, phone, followers)\n",
    "                    \n",
    "                    # Ensure we don't use meta.ai links - they're not real profile links\n",
    "                    if final_website and any(blacklisted in final_website.lower() for blacklisted in [\"meta.ai\", \"facebook.com/docs\"]):\n",
    "                        final_website = None\n",
    "                        \n",
    "                    # If we have other links in the list but not as final_website, use the first valid one\n",
    "                    if not final_website and all_links:\n",
    "                        for link in all_links:\n",
    "                            if not any(blacklisted in link.lower() for blacklisted in [\"meta.ai\", \"facebook.com/docs\"]):\n",
    "                                final_website = link\n",
    "                                break\n",
    "                    \n",
    "                    # Display results with proper \"None\" handling\n",
    "                    print(f\"Username: {username}\")\n",
    "                    print(f\"Display Name: {display_name or 'None'}\")\n",
    "                    print(f\"Followers: {followers}\")\n",
    "                    print(f\"Email: {email or 'None'}\")\n",
    "                    print(f\"Phone: {phone or 'None'}\")\n",
    "                    print(f\"Website: {final_website or 'None'}\")\n",
    "                    if all_links and len(all_links) > 0:\n",
    "                        print(f\" All detected links: {', '.join(all_links[:3])}\" + (f\" + {len(all_links) - 3} more\" if len(all_links) > 3 else \"\"))\n",
    "                    else:\n",
    "                        print(\" All detected links: None\")\n",
    "                    print(f\"Location: {location or 'None'}\")\n",
    "                    print(f\"Lead Score: {score}\")\n",
    "                    print(f\"Bio: {bio[:150]}...\" if len(bio) > 150 else f\" Bio: {bio or 'None'}\")\n",
    "                    \n",
    "                    # Save to database - store None instead of empty values\n",
    "                    cursor.execute('''\n",
    "                        INSERT OR IGNORE INTO profiles (\n",
    "                            username, email, phone, website_link, follower_count, location, lead_score, date_scraped\n",
    "                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    ''', (\n",
    "                        username, \n",
    "                        email or None, \n",
    "                        phone or None, \n",
    "                        final_website or None, \n",
    "                        followers or 0, \n",
    "                        location or None, \n",
    "                        score or 0,\n",
    "                        datetime.now().isoformat(sep=' ', timespec='seconds')\n",
    "                    ))\n",
    "                    conn.commit()\n",
    "                    \n",
    "                    print(f\" Saved @{username} (Score: {score}, Time: {profile_data.get('scrape_time', 0):.1f}s)\")\n",
    "                    \n",
    "                    # Dynamic delay based on scraping speed and success\n",
    "                    base_delay = 1.5\n",
    "                    if profile_data.get('scrape_time', 0) < 3:\n",
    "                        # If we scraped quickly, add slightly more delay to avoid suspicion\n",
    "                        delay = random.uniform(base_delay + 0.5, base_delay + 1.5)\n",
    "                    else:\n",
    "                        # If scraping took longer, reduce delay\n",
    "                        delay = random.uniform(base_delay, base_delay + 1)\n",
    "                    \n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\" Error scraping {username}: {e}\")\n",
    "                    # If we get rate limited, pause briefly then continue \n",
    "                    if \"Please wait\" in str(e) or \"rate limit\" in str(e).lower():\n",
    "                        print(\" Rate limit hit. Waiting 3 minutes...\")\n",
    "                        time.sleep(180)  # Reduced from 5 minutes to 3 minutes\n",
    "                    elif \"challenge\" in str(e).lower():\n",
    "                        print(\"️ Challenge detected. Waiting 5 minutes...\")\n",
    "                        time.sleep(300)  # 5 minutes for challenges\n",
    "            \n",
    "            # Sleep between batches to avoid rate limiting\n",
    "            if i + batch_size < len(usernames):\n",
    "                print(f\"⏸ Batch complete. Sleeping for {batch_delay // 60} min to avoid rate limits...\\n\")\n",
    "                time.sleep(batch_delay)\n",
    "        \n",
    "        print(\" All profiles enriched.\")\n",
    "        \n",
    "        # Summary\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM profiles\")\n",
    "        total = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute(\"SELECT COUNT(*) FROM profiles WHERE location IS NOT NULL\")\n",
    "        with_location = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute(\"SELECT COUNT(*) FROM profiles WHERE email IS NOT NULL OR phone IS NOT NULL\")\n",
    "        with_contact = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute(\"SELECT COUNT(*) FROM profiles WHERE website_link IS NOT NULL\")\n",
    "        with_website = cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"\\n Summary:\")\n",
    "        print(f\"   Total profiles: {total}\")\n",
    "        print(f\"   With location: {with_location} ({with_location/total*100:.1f}%)\" if total > 0 else \"   With location: 0\")\n",
    "        print(f\"   With contact: {with_contact} ({with_contact/total*100:.1f}%)\" if total > 0 else \"   With contact: 0\")\n",
    "        print(f\"   With website: {with_website} ({with_website/total*100:.1f}%)\" if total > 0 else \"   With website: 0\")\n",
    "        \n",
    "    finally:\n",
    "        # Close database and browser\n",
    "        conn.close()\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872acd2f-9dbe-4759-a89d-6362d61518fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconnect to the DB\n",
    "conn = sqlite3.connect('instagram_leads.db')  \n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Run your SELECT query\n",
    "cursor.execute(\"SELECT * FROM profiles\")\n",
    "results = cursor.fetchall()\n",
    "\n",
    "# Print each row\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c72df4-df38-40fb-9387-2659979833fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
